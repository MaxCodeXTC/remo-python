{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![remo_logo](assets/remo_normal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, Remo will be used to accelerate the process of building a transfer learning pipeline for the task of Image Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "EI72wutnsdyO",
    "outputId": "347b3820-f40b-4707-c6c9-666680d951fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import remo\n",
    "remo.set_viewer('jupyter')\n",
    "from pprint import pprint\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "\n",
    "- The dataset used in this example is a subset of the <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Flowers 102 Dataset</a>.\n",
    "- Run the next cell to download the data from s3, create a new folder and extract the files required.\n",
    "\n",
    "- The directory structure of the dataset is:\n",
    "\n",
    "    ```\n",
    "    ├── small_flowers\n",
    "        ├── images\n",
    "            ├── 0\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── 1\n",
    "                ├── image_3.jpg\n",
    "                ├── image_4.jpg\n",
    "                ├── ...\n",
    "        ├── annotations\n",
    "            ├── mapping.json\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataset will be downloaded from s3, and extracted into a new folder\n",
    "!wget https://s-3.s3-eu-west-1.amazonaws.com/small_flowers.zip\n",
    "!unzip small_flowers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "root_dir = \"./small_flowers\"\n",
    "path_to_images = os.path.join(root_dir, \"images\")\n",
    "path_to_annotations = os.path.join(root_dir, \"annotations\")\n",
    "mapping_json_path = os.path.join(path_to_annotations, \"mapping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Annotations and Tags from Folders**\n",
    "\n",
    "To generate an annotations from a folder of folders, the path to the root directory containing the class folders is passed to ```remo.generate_annotations_from_folders(). ``` \n",
    "\n",
    "To generate tags file, a dictionary mapping the tag to list of paths is passed to ```remo.generate_tags_from_folders()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_list = [os.path.basename(i) for i in glob.glob(str(path_to_images)+\"/**/*.jpg\", recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "# Definiing the train test split\n",
    "train_idx = round(len(im_list) * 0.8)\n",
    "valid_idx = train_idx + round(len(im_list) * 0.1)\n",
    "test_idx = valid_idx + round(len(im_list) * 0.1)\n",
    "\n",
    "# Tags Dictionary\n",
    "tags_dict = {\"train\" : im_list[0:train_idx], \"valid\" : im_list[train_idx:valid_idx], \"test\" : im_list[valid_idx:test_idx]}\n",
    "\n",
    "# Generate annotations file from folder of folders\n",
    "remo.generate_annotations_from_folders(path_to_images)\n",
    "# Generating Tags file\n",
    "remo.generate_tags_from_folders(tags_dictionary = tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The JSON file is provided in the dataset, and is then converted into a mapping dictionary.\n",
    "# cat_to_index : mapping between class_index -> class_label\n",
    "cat_to_index = dict(json.load(open(mapping_json_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Data to Remo**\n",
    "\n",
    "To add a dataset, you can use the ```remo.create_dataset()``` specifying the path to data and annotations. \n",
    "The class encoding is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "flowers = remo.create_dataset(name='flowers', \n",
    "                              local_files=[path_to_images, \"./train_test_valid_split.csv\"],\n",
    "                              annotation_task = \"Image classification\",\n",
    "                              class_encoding=cat_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view your data and labels using the Remo visual interface directly in the notebook, call the ```dataset.view()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_view](assets/dataset_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties. \n",
    "\n",
    "This can be done either using code, or via the visual interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(\"Dataset Statistics [{'AnnotationSet ID': 77, 'AnnotationSet name': 'Image \"\n \"classification', 'n_images': 140, 'n_classes': 3, 'n_objects': 0, \"\n \"'top_3_classes': [{'name': 'Hard-leaved pocket orchid', 'count': 60}, \"\n \"{'name': 'Canterbury bells', 'count': 40}, {'name': 'Pink primrose', \"\n \"'count': 40}], 'creation_date': None, 'last_modified_date': \"\n \"'2020-08-04T06:33:04.790286Z'}]\")\n"
    }
   ],
   "source": [
    "data_stats = flowers.get_annotation_statistics()\n",
    "pprint(\"Dataset Statistics {}\".format(data_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![view_annotations_stats](assets/view_annotation_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Annotations To File**\n",
    "\n",
    "Using the ```dataset.export_annotations_to_file()``` method, the annotations from Remo can be exported to a format of your choice.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.export_annotations_to_file(\"flowers_annotations.zip\", annotation_format=\"csv\", full_path=True, export_tags=True)\n",
    "!unzip flowers_annotations.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "A custom PyTorch ```Dataset``` object defined below is used to load data.\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "- **Path to Tags:** Path to Tags file for Train, Test, Validation split CSV generated by Remo\n",
    "- **Path to Annotations:** Path to Annotations CSV File (Format : file_name, class_name)\n",
    "- **Mapping:** Python dictionary containing mapping of class name and class index (Format : {\"class_name\" : \"class_index\"})\n",
    "- **transforms:** Transforms to be applied to the images before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from Class Name --> Index\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgrPCv6E8ipU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, annotations, train_test_valid_split, mapping, mode, transform=None):\n",
    "        # Pandas is used to read in the csv file into a DataFrame for data loading\n",
    "        self.data = pd.read_csv(annotations)\n",
    "        self.train_test_valid_split = pd.read_csv(train_test_valid_split)\n",
    "        self.data[\"tag\"] = self.train_test_valid_split[\"tag\"]\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.data_df = self.data[self.data[\"tag\"] == self.mode][[\"file_name\", \"classes\"]].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = int(self.mapping[self.data_df.loc[idx, 'classes'].lower()])\n",
    "        im_path = self.data_df.loc[idx, 'file_name']\n",
    "        \n",
    "        label_tensor = torch.as_tensor(labels-1, dtype=torch.long)\n",
    "        im = Image.open(im_path)\n",
    "    \n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "    \n",
    "        if self.mode == \"test\":\n",
    "            # For saving the predicitions, the file name is required\n",
    "            return {\"im\" : im, \"labels\": label_tensor, \"im_name\" : self.data_df.loc[idx, 'file_name']}\n",
    "        else:\n",
    "            return {\"im\" : im, \"labels\" : label_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "\n",
    "test_valid_transforms = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a ```DataLoader``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FlowerDataset(annotations=\"Image classification.csv\",\n",
    "                              train_test_valid_split=\"tags.csv\",\n",
    "                              transform=train_transforms,\n",
    "                              mapping=mapping,\n",
    "                              mode=\"train\")\n",
    "\n",
    "valid_dataset = FlowerDataset(annotations=\"Image classification.csv\",\n",
    "                            train_test_valid_split=\"tags.csv\",\n",
    "                            transform=test_valid_transforms,\n",
    "                            mapping=mapping,\n",
    "                            mode=\"valid\")\n",
    "\n",
    "test_dataset = FlowerDataset(annotations=\"Image classification.csv\",\n",
    "                             train_test_valid_split=\"tags.csv\",\n",
    "                             transform=test_valid_transforms,\n",
    "                             mapping=mapping,\n",
    "                             mode=\"test\")\n",
    "\n",
    "# If you face issues in operating systems like Windows, you can set num_workers=0.\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,  shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The pre-trained weights of the ```ResNet-18``` model with ImageNet are used in this tutorial.\n",
    "\n",
    "To train the model, the following details are to be specified.\n",
    "\n",
    "1. **Model:** The edited version of the pre-trained model.\n",
    "2. **Data Loaders:** The dictionary containing our training and validation dataloaders\n",
    "3. **Criterion:** The loss function used for training the network\n",
    "4. **Num_epochs:** The number of epochs for which we would like to train the network.\n",
    "5. **dataset_size:** an additional parameter which is used to correctly scale the loss, the method for this is specified in the DataLoader cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "1e193402cc1b4dbe97531c28ac93a8bb",
      "18d04c743bb4459d8a8235e8846b178e",
      "ec07bf4e45574e9ca39217f19a0adb48",
      "822d18ace03e4637892cf40ae0996e4a",
      "ca38e79005dd4612ae97645c53113ae2",
      "2690eec5fed040cd801e83c2f178d068",
      "b37da2626af94b649d19213f38e14d65",
      "403ac43de7454dd4b4dfdab3a61ffe14"
     ]
    },
    "colab_type": "code",
    "id": "lCxV4MgZ8fEs",
    "outputId": "325236d0-57df-49f3-b881-a81183de8559"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freezing the weights\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "\n",
    "# Replacing the final layer\n",
    "model.fc = nn.Sequential(nn.Linear(512, 256), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(p=0.5), \n",
    "                         nn.Linear(256, 102), \n",
    "                         nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "num_epochs = 1\n",
    "data_loaders = {\"train\" : train_loader, \"valid\": val_loader}\n",
    "dataset_size = {\"train\" : len(train_dataset), \"valid\" : len(valid_dataset)}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device) # This method pushes the model to the device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_usNSOE7Ao6n"
   },
   "outputs": [],
   "source": [
    "# The training loop trains the model for the total number of epochs,\n",
    "# an epoch is one complete pass over the entire dataset\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train() # This sets the model back to training after the validation step\n",
    "    print(\"Epoch Number {}\".format(epoch))\n",
    "\n",
    "    training_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0\n",
    "    correct_preds = 0\n",
    "    best_acc = 0\n",
    "    validation = 0.0\n",
    "    total = 0\n",
    "\n",
    "    \n",
    "    data_loader = tqdm.tqdm(data_loaders[\"train\"])\n",
    "    for x, data in enumerate(data_loader):\n",
    "        inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = training_loss / dataset_size[\"train\"]\n",
    "\n",
    "    print(\"Training Loss : {:.5f}\".format(epoch_loss))\n",
    "\n",
    "    val_data_loader = tqdm.tqdm(data_loaders[\"valid\"])\n",
    "    \n",
    "    # Validation step after every epoch\n",
    "    # The gradients are not required at inference time, hence the model is set to eval mode\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, data in enumerate(val_data_loader):\n",
    "            inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            _, index = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct_preds += (index == labels).sum().item()\n",
    "\n",
    "            validation += val_loss.item()\n",
    "\n",
    "        val_acc = 100 * (correct_preds / total)\n",
    "\n",
    "        print(\"Validation Loss : {:.5f}\".format(validation / dataset_size[\"valid\"]))\n",
    "        print(\"Validation Accuracy is: {:.2f}%\".format(val_acc))\n",
    "        \n",
    "        # The model is saved only if current validation accuracy is higher than the previous best accuracy\n",
    "        if best_acc < val_acc:\n",
    "            best_acc = val_acc\n",
    "            model_name = \"./saved_model_{:.2f}.pt\".format(best_acc)\n",
    "            torch.save(model, model_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oASN5m6Ibn9l"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_model = torch.load(model_name)\n",
    "\n",
    "test_model.eval()\n",
    "tk0 = tqdm.tqdm(test_loader)\n",
    "\n",
    "total = 0\n",
    "correct_preds = 0\n",
    "pred_list = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, data in enumerate(tk0):\n",
    "        single_im, label = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "        im_name = data[\"im_name\"]\n",
    "        \n",
    "        pred = test_model(single_im)\n",
    "\n",
    "        _, index = torch.max(pred, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct_preds += (index == label).sum().item()\n",
    "        \n",
    "        pred_list[os.path.basename(im_name[0])] = cat_to_index[str((index+1).item())]\n",
    "        \n",
    "df = pd.DataFrame(pred_list.items(), columns=['file_name', 'class_name'])\n",
    "with open(\"results.csv\", \"w\") as f:\n",
    "    df.to_csv(f, index=False)\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * (correct_preds / total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "For visualizing the predicted v/s original label in Remo, the predictions are added to a CSV, which is then added as an ```AnnotationSet``` to the testing_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.create_annotation_set(\"Image Classification\", name=\"model_predictions\", path_to_annotation_file=\"./results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results Comparison](assets/results_comparison.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "remo_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pt_36': conda)",
   "language": "python",
   "name": "python361064bitpt36condae3e1a72d28c3471788eb7ee1d404ab9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18d04c743bb4459d8a8235e8846b178e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e193402cc1b4dbe97531c28ac93a8bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec07bf4e45574e9ca39217f19a0adb48",
       "IPY_MODEL_822d18ace03e4637892cf40ae0996e4a"
      ],
      "layout": "IPY_MODEL_18d04c743bb4459d8a8235e8846b178e"
     }
    },
    "2690eec5fed040cd801e83c2f178d068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "403ac43de7454dd4b4dfdab3a61ffe14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822d18ace03e4637892cf40ae0996e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403ac43de7454dd4b4dfdab3a61ffe14",
      "placeholder": "​",
      "style": "IPY_MODEL_b37da2626af94b649d19213f38e14d65",
      "value": " 44.7M/44.7M [00:21&lt;00:00, 2.16MB/s]"
     }
    },
    "b37da2626af94b649d19213f38e14d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca38e79005dd4612ae97645c53113ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec07bf4e45574e9ca39217f19a0adb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2690eec5fed040cd801e83c2f178d068",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca38e79005dd4612ae97645c53113ae2",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
