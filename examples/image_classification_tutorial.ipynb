{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![remo_logo](assets/remo_normal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, Remo will be used to accelerate the process of building a transfer learning pipeline for the task of Image Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Specify path to Remo\n",
    "# Mac version\n",
    "local_path_to_repo =  '/home/harsha/Documents/remo-python'\n",
    "# Windows version\n",
    "#local_path_to_repo =  'C:/Users/crows/Documents/GitHub/remo-python'\n",
    "\n",
    "sys.path.insert(0, local_path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "EI72wutnsdyO",
    "outputId": "347b3820-f40b-4707-c6c9-666680d951fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "from pprint import pprint\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "import remo\n",
    "remo.set_viewer('jupyter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "\n",
    "- The dataset used in this example is a subset of the <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Flowers 102 Dataset</a>.\n",
    "- The next cell will download the data as a zip file and extract the files in a new folder.\n",
    "\n",
    "- The folder structure of the dataset is:\n",
    "\n",
    "    ```\n",
    "    ├── small_flowers\n",
    "        ├── images\n",
    "            ├── 1\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── 2\n",
    "                ├── image_3.jpg\n",
    "                ├── image_4.jpg\n",
    "                ├── ...\n",
    "            ├── ... \n",
    "        ├── annotations\n",
    "        ├── mapping.json\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n"
     ]
    }
   ],
   "source": [
    "# The dataset will be extracted in a new folder\n",
    "if not os.path.exists('small_flowers.zip'):\n",
    "    !wget https://s-3.s3-eu-west-1.amazonaws.com/small_flowers.zip\n",
    "    !unzip small_flowers.zip\n",
    "else:\n",
    "    print('Files already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "path_to_images =  './small_flowers/images/'\n",
    "mapping_json_path =  './small_flowers/mapping.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations\n",
    "\n",
    "We can easily generate annotations from a series of folders, by passing the root directory path to ```remo.generate_annotations_from_folders(). ``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remo.generate_annotations_from_folders(path_to_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the labels as strings rather than IDs, we can use a dictionary mapping the two of them.\n",
    "In the donwnloaded dataset, we havev JSON file containing this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_index =  dict(json.load(open(mapping_json_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To organise our images, we can also generate a list of annotation tags. \n",
    "Among other things, this allows to generate train / test splits without the need to move around image files. \n",
    "To do this, we just need to pass a dictionary that maps tags to the relevant images paths to the function ```remo.generate_tags_from_folders()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_list = [os.path.basename(i) for i in glob.glob(path_to_images+'/**/*.jpg', recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "# Definining the train test split\n",
    "train_idx = round(len(im_list) * 0.8)\n",
    "valid_idx = train_idx + round(len(im_list) * 0.1)\n",
    "test_idx  = valid_idx + round(len(im_list) * 0.1)\n",
    "\n",
    "# Creating a dictionary with tags\n",
    "tags_dict =  {'train' : im_list[0:train_idx], \n",
    "              'valid' : im_list[train_idx:valid_idx], \n",
    "              'test' : im_list[valid_idx:test_idx]}\n",
    "\n",
    "remo.generate_image_tags(tags_dictionary  = tags_dict, \n",
    "                         output_file_path = os.path.join(path_to_images, 'images_tags.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset\n",
    "\n",
    "To add a dataset, you can use the ```remo.create_dataset()``` specifying the path to data and annotations.\n",
    "\n",
    "The class encoding is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "flowers =  remo.create_dataset(name = 'flowers', \n",
    "                              local_files = [path_to_images],\n",
    "                              annotation_task = 'Image classification',\n",
    "                              class_encoding = cat_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view your data and labels using the Remo visual interface directly in the notebook, call the ```dataset.view()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_view](assets/flower_dataset_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties. \n",
    "\n",
    "This can be done either using code, or via the visual interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AnnotationSet ID': 226,\n",
       "  'AnnotationSet name': 'Image classification',\n",
       "  'n_images': 140,\n",
       "  'n_classes': 3,\n",
       "  'n_objects': 0,\n",
       "  'top_3_classes': [{'name': 'Hard-leaved pocket orchid', 'count': 60},\n",
       "   {'name': 'Canterbury bells', 'count': 40},\n",
       "   {'name': 'Pink primrose', 'count': 40}],\n",
       "  'creation_date': None,\n",
       "  'last_modified_date': '2020-08-19T06:13:08.258898Z'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers.get_annotation_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![view_annotations_stats](assets/flower_dataset_view_annotation_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Annotations**\n",
    "\n",
    "Using the ```dataset.export_annotations_to_file()``` method, annotations can be exported to a format of your choice.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.export_annotations_to_file('flowers_annotations.zip', \n",
    "                                   annotation_format = 'csv', \n",
    "                                   full_path = True, \n",
    "                                   export_tags = True)\n",
    "\n",
    "annotation_for_model_path = './small_flowers/annotations/for_model/'\n",
    "   \n",
    "if not os.path.exists(annotation_for_model_path):\n",
    "    os.makedirs(annotation_for_model_path)\n",
    "    \n",
    "!unzip flowers_annotations.zip -d './small_flowers/annotations/for_model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "A custom PyTorch ```Dataset``` object defined below is used to load data.\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "- **Path to Tags:** Path to tags csv file for Train, Test, Validation split generated by Remo\n",
    "- **Path to Annotations:** Path to Annotations csv file (Format : file_name, class_name)\n",
    "- **Mapping:** Python dictionary containing mapping of class name and class index (Format : {'class_name' : 'class_index\"})\n",
    "- **transforms:** Transforms to be applied to the images before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from Class Name --> Index\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgrPCv6E8ipU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations, train_test_valid_split, mapping, mode, transform=None):\n",
    "        # Pandas is used to read in the csv file into a DataFrame for data loading\n",
    "        self.data = pd.read_csv(annotations)\n",
    "        self.train_test_valid_split = pd.read_csv(train_test_valid_split)\n",
    "        self.data['tag'] = self.train_test_valid_split['tag']\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        i_index = self.data['tag'] == self.mode\n",
    "        self.data_df = self.data[i_index][['file_name', 'classes']].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = int(self.mapping[self.data_df.loc[idx, 'classes'].lower()])\n",
    "        im_path = self.data_df.loc[idx, 'file_name']\n",
    "        \n",
    "        label_tensor =  torch.as_tensor(labels-1, dtype=torch.long)\n",
    "        im = Image.open(im_path)\n",
    "    \n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "    \n",
    "        if self.mode == 'test':\n",
    "            # For saving the predicitions, the file name is required\n",
    "            return {'im' : im, 'labels': label_tensor, 'im_name' : self.data_df.loc[idx, 'file_name']}\n",
    "        else:\n",
    "            return {'im' : im, 'labels' : label_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means =  [0.485, 0.456, 0.406]\n",
    "stds  =  [0.229, 0.224, 0.225]\n",
    "\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms =  transforms.Compose([\n",
    "                    transforms.RandomRotation(30),\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(means, stds)])\n",
    "\n",
    "test_valid_transforms =  transforms.Compose([\n",
    "                         transforms.Resize(224),\n",
    "                         transforms.CenterCrop(224),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a ```DataLoader``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FlowerDataset(annotations =  annotation_for_model_path + 'Image classification.csv',\n",
    "                              train_test_valid_split =  annotation_for_model_path + 'tags.csv',\n",
    "                              transform =  train_transforms,\n",
    "                              mapping =  mapping,\n",
    "                              mode =  'train')\n",
    "\n",
    "valid_dataset = FlowerDataset(annotations = annotation_for_model_path + 'Image classification.csv',\n",
    "                              train_test_valid_split = annotation_for_model_path + 'tags.csv',\n",
    "                              transform = test_valid_transforms,\n",
    "                              mapping = mapping,\n",
    "                              mode = 'valid')\n",
    "\n",
    "test_dataset  = FlowerDataset(annotations = annotation_for_model_path + 'Image classification.csv',\n",
    "                              train_test_valid_split = annotation_for_model_path + 'tags.csv',\n",
    "                              transform = test_valid_transforms,\n",
    "                              mapping = mapping,\n",
    "                              mode = 'test')\n",
    "\n",
    "# If you face issues in operating systems like Windows, you can set num_workers=0.\n",
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader   =  torch.utils.data.DataLoader(valid_dataset, batch_size=1,  shuffle=False, num_workers=2)\n",
    "test_loader  =  torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We use a ```ResNet-18``` architecture, with weight pre-trained on ImageNet.\n",
    "\n",
    "To train the model, the following details are to be specified:\n",
    "\n",
    "1. **Model:** The edited version of the pre-trained model.\n",
    "2. **Data Loaders:** The dictionary containing our training and validation dataloaders\n",
    "3. **Criterion:** The loss function used for training the network\n",
    "4. **Num_epochs:** The number of epochs for which we would like to train the network.\n",
    "5. **dataset_size:** an additional parameter which is used to correctly scale the loss, the method for this is specified in the DataLoader cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "1e193402cc1b4dbe97531c28ac93a8bb",
      "18d04c743bb4459d8a8235e8846b178e",
      "ec07bf4e45574e9ca39217f19a0adb48",
      "822d18ace03e4637892cf40ae0996e4a",
      "ca38e79005dd4612ae97645c53113ae2",
      "2690eec5fed040cd801e83c2f178d068",
      "b37da2626af94b649d19213f38e14d65",
      "403ac43de7454dd4b4dfdab3a61ffe14"
     ]
    },
    "colab_type": "code",
    "id": "lCxV4MgZ8fEs",
    "outputId": "325236d0-57df-49f3-b881-a81183de8559"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freezing the weights\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "\n",
    "# Replacing the final layer\n",
    "model.fc =  nn.Sequential(nn.Linear(512, 256), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(p=0.5), \n",
    "                         nn.Linear(256, 102), \n",
    "                         nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "\n",
    "optimizer    =  optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "criterion    =  nn.NLLLoss()\n",
    "num_epochs   =  1\n",
    "data_loaders =  {'train' : train_loader, 'valid': val_loader}\n",
    "dataset_size =  {'train' : len(train_dataset), 'valid' : len(valid_dataset)}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device) # This method pushes the model to the device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_usNSOE7Ao6n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training loop trains the model for the total number of epochs.\n",
    "# An epoch is one complete pass over the entire dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train() # This sets the model back to training after the validation step\n",
    "    print('\\nEpoch Number {}''.format(epoch+1))\n",
    "\n",
    "    training_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0\n",
    "    correct_preds = 0\n",
    "    best_acc = 0\n",
    "    validation = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    data_loader = tqdm.tqdm(data_loaders['train'])\n",
    "    \n",
    "    for x, data in enumerate(data_loader):\n",
    "        inputs, labels = data['im'].to(device), data['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = training_loss / dataset_size[\"train\"]\n",
    "    print('Training Loss : {:.5f}'.format(epoch_loss))\n",
    "    val_data_loader = tqdm.tqdm(data_loaders['valid'])\n",
    "    \n",
    "    # Validation step after every epoch\n",
    "    # The gradients are not required at inference time, hence the model is set to eval mode\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, data in enumerate(val_data_loader):\n",
    "            inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            _, index = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct_preds += (index == labels).sum().item()\n",
    "\n",
    "            validation += val_loss.item()\n",
    "\n",
    "        val_acc = 100 * (correct_preds / total)\n",
    "\n",
    "        print('Validation Loss : {:.5f}'.format(validation / dataset_size['valid']))\n",
    "        print('Validation Accuracy is: {:.2f}%'format(val_acc))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oASN5m6Ibn9l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.eval()\n",
    "tk0 =  tqdm.tqdm(test_loader)\n",
    "\n",
    "total =  0\n",
    "correct_preds =  0\n",
    "pred_list =  {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, data in enumerate(tk0):\n",
    "        single_im, label = data['im'].to(device), data['labels'].to(device)\n",
    "        im_name = data['im_name']\n",
    "        \n",
    "        pred = model(single_im)\n",
    "\n",
    "        _, index = torch.max(pred, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct_preds += (index == label).sum().item()\n",
    "        \n",
    "        pred_list[os.path.basename(im_name[0])] = cat_to_index[str((index+1).item())]\n",
    "        \n",
    "df = pd.DataFrame(pred_list.items(), columns=['file_name', 'class_name'])\n",
    "with open('./small_flowers/results.csv', 'w') as f:\n",
    "    df.to_csv(f, index=False)\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * (correct_preds / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Using Remo, we can visualize predictions  vs the original labels.\n",
    "\n",
    "To do this we create a new ```AnnotationSet```, and  upload predictions as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.create_annotation_set(annotation_task = 'Image Classification', \n",
    "                              name = 'model_predictions', \n",
    "                              path_to_annotation_file = './small_flowers/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results Comparison](assets/flower_dataset_view_predictions.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "remo_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pt_36': conda)",
   "language": "python",
   "name": "python361064bitpt36condae3e1a72d28c3471788eb7ee1d404ab9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18d04c743bb4459d8a8235e8846b178e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e193402cc1b4dbe97531c28ac93a8bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec07bf4e45574e9ca39217f19a0adb48",
       "IPY_MODEL_822d18ace03e4637892cf40ae0996e4a"
      ],
      "layout": "IPY_MODEL_18d04c743bb4459d8a8235e8846b178e"
     }
    },
    "2690eec5fed040cd801e83c2f178d068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "403ac43de7454dd4b4dfdab3a61ffe14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822d18ace03e4637892cf40ae0996e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403ac43de7454dd4b4dfdab3a61ffe14",
      "placeholder": "​",
      "style": "IPY_MODEL_b37da2626af94b649d19213f38e14d65",
      "value": " 44.7M/44.7M [00:21&lt;00:00, 2.16MB/s]"
     }
    },
    "b37da2626af94b649d19213f38e14d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca38e79005dd4612ae97645c53113ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec07bf4e45574e9ca39217f19a0adb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2690eec5fed040cd801e83c2f178d068",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca38e79005dd4612ae97645c53113ae2",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}