{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, we will use Remo to accelerate and improve the process of building a transfer learning pipeline for an Object Detection task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we will:\n",
    "- Use Remo to browse through our images and annotations\n",
    "- Use Remo to understand the properties of the dataset and annotations by visualizing statistics.\n",
    "- Create a custom train, test, valid split in-place using Remo image tags.\n",
    "- Fine tune a pre-trained FasterRCNN model from torchvision and do some inference\n",
    "- Visually compare bounding box predictions with the ground truth\n",
    "\n",
    "**Along the way, we will see how the Dataset visualization provided Remo helps to gather insights to improve the dataset and the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "random.seed(4)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import remo\n",
    "remo.set_viewer('jupyter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "- The dataset used in this example is a subset of the [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html).\n",
    "\n",
    "- The directory structure of the dataset is:\n",
    "\n",
    "        ├── object_detection_dataset\n",
    "            ├── images\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── annotations\n",
    "                ├── annotations.csv\n",
    "                ├── model_predictions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2020-09-01 11:07:30--  https://s-3.s3-eu-west-1.amazonaws.com/object_detection_dataset.zip\n",
      "Resolving s-3.s3-eu-west-1.amazonaws.com (s-3.s3-eu-west-1.amazonaws.com)... 52.218.109.88\n",
      "Connecting to s-3.s3-eu-west-1.amazonaws.com (s-3.s3-eu-west-1.amazonaws.com)|52.218.109.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2041716 (1.9M) [application/zip]\n",
      "Saving to: 'object_detection_dataset.zip'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2%  143K 14s\n",
      "    50K .......... .......... .......... .......... ..........  5%  432K 9s\n",
      "   100K .......... .......... .......... .......... ..........  7% 1.18M 6s\n",
      "   150K .......... .......... .......... .......... .......... 10%  301K 6s\n",
      "   200K .......... .......... .......... .......... .......... 12%  510K 5s\n",
      "   250K .......... .......... .......... .......... .......... 15%  445K 5s\n",
      "   300K .......... .......... .......... .......... .......... 17%  455K 5s\n",
      "   350K .......... .......... .......... .......... .......... 20% 1.08M 4s\n",
      "   400K .......... .......... .......... .......... .......... 22%  342K 4s\n",
      "   450K .......... .......... .......... .......... .......... 25%  319K 4s\n",
      "   500K .......... .......... .......... .......... .......... 27% 1.15M 4s\n",
      "   550K .......... .......... .......... .......... .......... 30%  520K 3s\n",
      "   600K .......... .......... .......... .......... .......... 32% 1.13M 3s\n",
      "   650K .......... .......... .......... .......... .......... 35%  414K 3s\n",
      "   700K .......... .......... .......... .......... .......... 37%  535K 3s\n",
      "   750K .......... .......... .......... .......... .......... 40%  523K 3s\n",
      "   800K .......... .......... .......... .......... .......... 42%  763K 3s\n",
      "   850K .......... .......... .......... .......... .......... 45% 2.44M 2s\n",
      "   900K .......... .......... .......... .......... .......... 47%  387K 2s\n",
      "   950K .......... .......... .......... .......... .......... 50%  299K 2s\n",
      "  1000K .......... .......... .......... .......... .......... 52% 17.0M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 55% 26.5M 2s\n",
      "  1100K .......... .......... .......... .......... .......... 57%  306K 2s\n",
      "  1150K .......... .......... .......... .......... .......... 60% 2.29M 2s\n",
      "  1200K .......... .......... .......... .......... .......... 62%  397K 2s\n",
      "  1250K .......... .......... .......... .......... .......... 65%  455K 1s\n",
      "  1300K .......... .......... .......... .......... .......... 67%  508K 1s\n",
      "  1350K .......... .......... .......... .......... .......... 70% 1.16M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 72%  468K 1s\n",
      "  1450K .......... .......... .......... .......... .......... 75% 1.56M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 77%  313K 1s\n",
      "  1550K .......... .......... .......... .......... .......... 80%  374K 1s\n",
      "  1600K .......... .......... .......... .......... .......... 82% 1.05M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 85%  626K 1s\n",
      "  1700K .......... .......... .......... .......... .......... 87%  753K 0s\n",
      "  1750K .......... .......... .......... .......... .......... 90%  791K 0s\n",
      "  1800K .......... .......... .......... .......... .......... 92%  550K 0s\n",
      "  1850K .......... .......... .......... .......... .......... 95% 1.34M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 97%  529K 0s\n",
      "  1950K .......... .......... .......... .......... ...       100% 1.13M=3.7s\n",
      "\n",
      "2020-09-01 11:07:34 (535 KB/s) - 'object_detection_dataset.zip' saved [2041716/2041716]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The dataset will be extracted in a new folder\n",
    "if not os.path.exists('object_detection_dataset.zip'):\n",
    "    !wget https://s-3.s3-eu-west-1.amazonaws.com/object_detection_dataset.zip\n",
    "    !unzip -qq object_detection_dataset.zip\n",
    "else:\n",
    "    print('Files already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "path_to_images =  './object_detection_dataset/images/'\n",
    "path_to_annotations = './object_detection_dataset/annotations/'\n",
    "\n",
    "annotations_file_path = os.path.join(path_to_annotations, 'annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the labels as strings rather than IDs, we can use a dictionary mapping the two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between Class name and Index\n",
    "cat_to_index = {'Wheel'        : 1, \n",
    "                'Car'          : 2,\n",
    "                'Person'       : 3, \n",
    "                'Land vehicle' : 4, \n",
    "                'Human body'   : 5, \n",
    "                'Plant'        : 6, \n",
    "                'Tire'         : 7, \n",
    "                'Vehicle'      : 8, \n",
    "                'Vehicle registration plate' : 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Remo, we can use tags to organise our images.\n",
    "Among other things, this allows us to generate train / test splits without the need to move image files around.\n",
    "\n",
    "To do this, we just need to pass a dictionary (mapping tags to the relevant images paths) to the function \n",
    "```remo.generate_image_tags()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./object_detection_dataset/annotations/images_tags.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_list = [os.path.abspath(i) for i in glob.glob(path_to_images + '/**/*.jpg', recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "# Definining the train test split\n",
    "train_idx = round(len(im_list) * 0.4)\n",
    "valid_idx = train_idx + round(len(im_list) * 0.3)\n",
    "test_idx  = valid_idx + round(len(im_list) * 0.3)\n",
    "\n",
    "# Creating a dictionary with tags\n",
    "tags_dict =  {'train' : im_list[0:train_idx], \n",
    "              'valid' : im_list[train_idx:valid_idx], \n",
    "              'test' : im_list[valid_idx:test_idx]}\n",
    "\n",
    "train_test_split_file_path = os.path.join(path_to_annotations, 'images_tags.csv') \n",
    "remo.generate_image_tags(tags_dictionary  = tags_dict, \n",
    "                         output_file_path = train_test_split_file_path, \n",
    "                         append_path = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "\n",
    "To create a dataset we can use ```remo.create_dataset()```, specifying the path to data and annotations.\n",
    "\n",
    "The class encoding (if required) is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images: 7 of 7 files                                                                      Processing data - completed                                                                          \n",
      "Data upload completed\n"
     ]
    }
   ],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "object_detection_dataset =  remo.create_dataset(name = 'object-detection-dataset', \n",
    "                                                local_files = [path_to_images, path_to_annotations],\n",
    "                                                annotation_task = 'Object Detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view and explore images and labels, we can use Remo directly from the notebook. We just need to call ```dataset.view()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open http://localhost:8123/datasets/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            id=\"remo_frame_a5085100-ec7a-4dd3-ba5f-1fbb502747d9\"\n",
       "            width=\"100%\"\n",
       "            height=\"100px\"\n",
       "            src=\"http://localhost:8123/datasets/2?allheadless\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        <script type=\"text/javascript\">\n",
       "            (function () {\n",
       "                const iframe = document.getElementById(\"remo_frame_a5085100-ec7a-4dd3-ba5f-1fbb502747d9\");\n",
       "                let timeout, delay = 100;\n",
       "            \n",
       "                const setHeight = () => {\n",
       "                  const width = iframe.clientWidth;\n",
       "                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n",
       "                }\n",
       "                window.addEventListener(\"resize\", () => {\n",
       "                    clearTimeout(timeout);\n",
       "                  // start timing for event \"completion\"\n",
       "                  timeout = setTimeout(setHeight, delay);\n",
       "                });\n",
       "                setHeight();\n",
       "            })()\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_detection_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_view](assets/obj_dataset_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataset, we notice some interesting points:\n",
    "- There is a significant degree of overlap in bounding boxes of different classes (e.g. Wheel and Car)\n",
    "- Bounding box sizes vary a good amount across Wheel and Car objects\n",
    "- Pictures of Cars can be taken from different angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Using Remo, we can quickly visualize some key Dataset properties that can help us with our modelling, without needing to write extra boilerplate code.\n",
    "\n",
    "This can be done either from code, or using the visual interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AnnotationSet ID': 3,\n",
       "  'AnnotationSet name': 'Object detection',\n",
       "  'n_images': 7,\n",
       "  'n_classes': 9,\n",
       "  'n_objects': 40,\n",
       "  'top_3_classes': [{'name': 'Wheel', 'count': 18},\n",
       "   {'name': 'Car', 'count': 9},\n",
       "   {'name': 'Tire', 'count': 4}],\n",
       "  'creation_date': None,\n",
       "  'last_modified_date': '2020-09-01T09:08:00.399162Z'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_detection_dataset.get_annotation_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open http://localhost:8123/annotation-detail/3/insights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            id=\"remo_frame_650be9d3-8bd7-4c0f-b954-3f6a63922ade\"\n",
       "            width=\"100%\"\n",
       "            height=\"100px\"\n",
       "            src=\"http://localhost:8123/annotation-detail/3/insights?allheadless\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        <script type=\"text/javascript\">\n",
       "            (function () {\n",
       "                const iframe = document.getElementById(\"remo_frame_650be9d3-8bd7-4c0f-b954-3f6a63922ade\");\n",
       "                let timeout, delay = 100;\n",
       "            \n",
       "                const setHeight = () => {\n",
       "                  const width = iframe.clientWidth;\n",
       "                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n",
       "                }\n",
       "                window.addEventListener(\"resize\", () => {\n",
       "                    clearTimeout(timeout);\n",
       "                  // start timing for event \"completion\"\n",
       "                  timeout = setTimeout(setHeight, delay);\n",
       "                });\n",
       "                setHeight();\n",
       "            })()\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_detection_dataset.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![obj_view_annotations_stats](assets/obj_view_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the statistics we can gain some useful insights** like: \n",
    "\n",
    "- Some labels are not present in the test and valid set, but are present in the training set. This means we will not get an indicative model performance for these class (which is fine for the tutorial's sake, but in real life we would want to fix that)\n",
    "\n",
    "\n",
    "- The Wheel class has more instances than any other class in the dataset. Higher reported performance on this class might be caused by this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "Here we start working with PyTorch. To load the data, we will define a custom PyTorch ```Dataset``` object (as usual with PyTorch).\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "- **Path to Tags:** path to tags csv file for Train, Test, Validation split. Format: file_name, tag\n",
    "- **Path to Annotations:** path to the annotations CSV File. Format : file_name, classes, xmin, ymin, xmax, ymax\n",
    "- **(Optional) Mapping:** a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do\n",
    "\n",
    "-> add information the parameters to the constructor below\n",
    "\n",
    "-> edit the information above to include the specific variables that need changing for custom datasets\n",
    "(can the same be applied to img classification tutorial?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 annotations, \n",
    "                 train_test_valid_split, \n",
    "                 image_dir, \n",
    "                 mapping = None, \n",
    "                 mode = 'train', \n",
    "                 transform = None):\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        # Loading the annotation file (same format as Remo's)\n",
    "        data = pd.read_csv(annotations, index_col='file_name')\n",
    "        # Loading the train/test split file (same format as Remo's)\n",
    "        data['tag'] = pd.read_csv(train_test_valid_split, index_col='file_name')\n",
    "        data = data.reset_index()\n",
    "        \n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load only Train/Test/Split depending on the mode\n",
    "        i_index = self.data['tag'] == self.mode\n",
    "        self.data = self.data[i_index].reset_index(drop=True)\n",
    "        self.file_names = self.data['file_name'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.file_names.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        file_name = self.file_names[index]\n",
    "        records = self.data[self.data['file_name'] == file_name].reset_index()       \n",
    "        image = np.array(Image.open(f'{self.image_dir}/{file_name}'), dtype=np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        if self.mode != 'test':\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "            \n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            if self.mapping is not None:\n",
    "                labels = np.zeros((records.shape[0],))\n",
    "            \n",
    "                for i in range(records.shape[0]):\n",
    "                    labels[i] = self.mapping[records.loc[i, 'classes']]\n",
    "                    \n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            else:\n",
    "                labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd \n",
    "            target['boxes'] = torch.stack(list((map(torch.tensor, target['boxes'])))).type(torch.float32)\n",
    "\n",
    "            return image, target, file_name\n",
    "        else:\n",
    "            return image, file_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a ```DataLoader``` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Here the operations provided with Remo are integrated into a workflow in PyTorch by using the custom ObjectDetectionDataset method.\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(annotations = annotations_file_path,  \n",
    "                                       train_test_valid_split = train_test_split_file_path,\n",
    "                                       image_dir = path_to_images, \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'train')\n",
    "\n",
    "test_dataset = ObjectDetectionDataset(annotations = annotations_file_path,  \n",
    "                                       train_test_valid_split = train_test_split_file_path, \n",
    "                                       image_dir = path_to_images,\n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'test')\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_data_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "In this tutorial, we use a ```Faster RCNN``` architecture with a ```ResNet-50 Backbone```, pre-trained on on COCO train2017. This is [loaded directly from torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html#faster-r-cnn)\n",
    "\n",
    "To train the model, we specify the foolowing details:\n",
    "\n",
    "- **Model**: The edited version of the pre-trained model.\n",
    "- **num_classes**: The number of classes present in the dataset = actual n of classes + 1 for background of the image (that's a peculiarity of Faster RCNN)\n",
    "- **Optimizer:** The optimizer used for training the network\n",
    "- **Num_epochs:** The number of epochs for which we would like to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 10\n",
    "loss_value  = 0.0\n",
    "num_epochs  = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training loop trains the model for the total number of epochs.\n",
    "# (1 epoch = one complete pass over the entire dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_data_loader = tqdm.tqdm(train_data_loader)\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "    print('\\nTraining Loss : {:.5f}'.format(loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Using Remo, we can easily iterate through the images to compare the model predictions against the original labels.\n",
    "\n",
    "To do this, we just need to upload the model predictions to a new ```AnnotationSet```, which we call `model_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping Between Predicted Index and Class Name\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}\n",
    "\n",
    "detection_threshold = 0.3\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "test_data_loader = tqdm.tqdm(test_data_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in test_data_loader:\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "\n",
    "            boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "            scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "            boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "            scores = scores[scores >= detection_threshold]\n",
    "            image_id = image_ids[i]\n",
    "            \n",
    "            for box, labels in zip(boxes, outputs[i]['labels']):\n",
    "                results.append({'file_name' : os.path.basename(image_id), \n",
    "                                'classes'   : mapping[labels.item()], \n",
    "                                'xmin'      : box[0],\n",
    "                                'ymin'      : box[1],\n",
    "                                'xmax'      : box[2],\n",
    "                                'ymax'      : box[3]})\n",
    "\n",
    "model_predictions_path = path_to_annotations + 'model_predictions.csv'\n",
    "            \n",
    "with open(model_predictions_path, 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['file_name', 'classes', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = object_detection_dataset.create_annotation_set(annotation_task='Object Detection', \n",
    "                                                             name = 'model-predictions-oid',\n",
    "                                                             paths_to_files = [train_test_split_file_path, model_predictions_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualizing the predicted boxes against the ground truth, we can get visual feedback on how the model is performing and gain some insights.\n",
    "\n",
    "Via this workflow, we can not only understand the model through its metrics, but also visually inspect its biases and iterate to improve the model.\n",
    "\n",
    "Some insights include, how the model makes an incorrect prediction of the the lamp as a wheel, perhaps due to the shape being similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_predictions](assets/obj_visualize_results.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
