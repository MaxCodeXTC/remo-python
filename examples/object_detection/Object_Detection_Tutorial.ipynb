{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, Remo will be used to accelerate the process of building a transfer learning pipeline for the task of Object Detection. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we will see how:\n",
    "- To use Remo to visualize and explore annotations in an Object Detection Task\n",
    "- Use Remo to understand the properties of the dataset and annotations by visualizing statistics.\n",
    "- Create a custom train, test, valid split of an Object Detection dataset in-place using Remo image tags.\n",
    "- Fine tune and infer using a pre-trained FasterRCNN model from torchvision.\n",
    "- Visually compare bounding box predictions with the ground truth, and get insights to improve the dataset and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Specify path to Remo\n",
    "# Mac version\n",
    "local_path_to_repo =  '/home/harsha/Documents/rediscovery/remo-python'\n",
    "# Windows version\n",
    "#local_path_to_repo =  'C:/Users/crows/Documents/GitHub/remo-python'\n",
    "\n",
    "sys.path.insert(0, local_path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "random.seed(4)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import remo\n",
    "remo.set_viewer('jupyter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "- The dataset used in this example is a subset of the [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html).\n",
    "\n",
    "- The directory structure of the dataset is:\n",
    "\n",
    "        ├── object_detection_dataset\n",
    "            ├── images\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── annotations\n",
    "                ├── annotations.csv\n",
    "                ├── model_predictions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataset will be extracted in a new folder\n",
    "if not os.path.exists('object_detection_dataset.zip'):\n",
    "    !wget https://s-3.s3-eu-west-1.amazonaws.com/object_detection_dataset.zip\n",
    "    !unzip -qq object_detection_dataset.zip\n",
    "else:\n",
    "    print('Files already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "path_to_images =  './object_detection_dataset/images/'\n",
    "path_to_annotations = './object_detection_dataset/annotations/'\n",
    "\n",
    "annotations_file_path = os.path.join(path_to_annotations, 'annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the labels as strings rather than IDs, we can use a dictionary mapping the two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between Class name and Index\n",
    "cat_to_index = {'Wheel'        : 1, \n",
    "                'Car'          : 2,\n",
    "                'Person'       : 3, \n",
    "                'Land vehicle' : 4, \n",
    "                'Human body'   : 5, \n",
    "                'Plant'        : 6, \n",
    "                'Tire'         : 7, \n",
    "                'Vehicle'      : 8, \n",
    "                'Vehicle registration plate' : 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Remo, we can use tags to organise our images.\n",
    "Among other things, this allows us to generate train / test splits without the need to move image files around.\n",
    "\n",
    "To do this, we just need to pass a dictionary (mapping tags to the relevant images paths) to the function \n",
    "```remo.generate_image_tags()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_list = [os.path.abspath(i) for i in glob.glob(path_to_images + '/**/*.jpg', recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "# Definining the train test split\n",
    "train_idx = round(len(im_list) * 0.6)\n",
    "valid_idx = train_idx + round(len(im_list) * 0.2)\n",
    "test_idx  = valid_idx + round(len(im_list) * 0.2)\n",
    "\n",
    "# Creating a dictionary with tags\n",
    "tags_dict =  {'train' : im_list[0:train_idx], \n",
    "              'valid' : im_list[train_idx:valid_idx], \n",
    "              'test' : im_list[valid_idx:test_idx]}\n",
    "\n",
    "train_test_split_file_path = os.path.join(path_to_annotations, 'images_tags.csv') \n",
    "remo.generate_image_tags(tags_dictionary  = tags_dict, \n",
    "                         output_file_path = train_test_split_file_path, \n",
    "                         append_path = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "\n",
    "To create a dataset we can use ```remo.create_dataset()```, specifying the path to data and annotations.\n",
    "\n",
    "The class encoding (if required) is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "object_detection_dataset =  remo.create_dataset(name = 'ground-truth-oid', \n",
    "                                                local_files = [path_to_images, path_to_annotations],\n",
    "                                                annotation_task = 'Object Detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view and explore images and labels, we can use Remo directly from the notebook. We just need to call ```dataset.view()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_view](assets/obj_dataset_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization of the dataset, we notice some interesting points:\n",
    "- There is a significant degree of overlap between the classes\n",
    "- The size difference of the box between that of the wheel and car is quite large\n",
    "- The orientation of each car in the dataset is different relative to the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties.\n",
    "\n",
    "This can be done either using code, or via the visual interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![view_annotations_stats](assets/obj_view_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics view, provide some insights \n",
    "- Some labels are not present in the test and valid set, but are present in the training set.\n",
    "- The wheel class has more instances as compared to any other class in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "Here we start working with PyTorch. To load data, we will define a custom PyTorch ```Dataset``` object (as usual with PyTorch).\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "- **Path to Tags:** path to tags csv file for Train, Test, Validation split. Format: file_name, tag\n",
    "- **Path to Annotations:** Path to Annotations CSV File (Format : file_name, classes, xmin, ymin, xmax, ymax)\n",
    "- **(Optional) Mapping:** a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, annotations, train_test_valid_split, image_dir, mapping = None, mode = 'train', transform = None):\n",
    "        self.image_dir = image_dir\n",
    "        self.data = pd.read_csv(annotations)\n",
    "        self.data = self.data.set_index('file_name')\n",
    "        self.train_test_valid_split = pd.read_csv(train_test_valid_split).set_index('file_name')\n",
    "        self.data['tag'] = -1 \n",
    "        \n",
    "        self.data.update(self.train_test_valid_split)\n",
    "        self.data = self.data.reset_index()\n",
    "        \n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load only Train/Test/Split depending on the mode\n",
    "        i_index = self.data['tag'] == self.mode\n",
    "        self.data = self.data[i_index].reset_index(drop=True)\n",
    "        \n",
    "        self.file_names = self.data['file_name'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.file_names.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        file_name = self.file_names[index]\n",
    "        records = self.data[self.data['file_name'] == file_name].reset_index()\n",
    "        \n",
    "        image = np.array(Image.open(f'{self.image_dir}/{file_name}'), dtype=np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        if self.mode != 'test':\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "            \n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            if self.mapping is not None:\n",
    "                labels = np.zeros((records.shape[0],))\n",
    "            \n",
    "                for i in range(records.shape[0]):\n",
    "                    labels[i] = self.mapping[records.loc[i, 'classes']]\n",
    "                    \n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            else:\n",
    "                labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "            \n",
    "            target = {}\n",
    "\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd \n",
    "            target['boxes'] = torch.stack(list((map(torch.tensor, target['boxes'])))).type(torch.float32)\n",
    "\n",
    "            return image, target, file_name\n",
    "        else:\n",
    "            return image, file_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a DataLoader method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(annotations = annotations_file_path,  \n",
    "                                       train_test_valid_split = train_test_split_file_path,\n",
    "                                       image_dir = path_to_images, \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'train')\n",
    "\n",
    "test_dataset = ObjectDetectionDataset(annotations = annotations_file_path,  \n",
    "                                       train_test_valid_split = train_test_split_file_path, \n",
    "                                       image_dir = path_to_images,\n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'test')\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_data_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The pre-trained ```Faster RCNN``` Model with the ```ResNet-50 Backbone``` is used in this tutorial.\n",
    "\n",
    "To train the model, the following details are specified:\n",
    "\n",
    "- **Model**: The edited version of the pre-trained model.\n",
    "- **num_classes**: The number of classes present in the dataset (Eg: num_classes + 1 (background))\n",
    "- **Optimizer:** The optimizer used for training the network\n",
    "- **Num_epochs:** The number of epochs for which we would like to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 10\n",
    "loss_value  = 0.0\n",
    "num_epochs  = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training loop trains the model for the total number of epochs.\n",
    "# (1 epoch = one complete pass over the entire dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_data_loader = tqdm.tqdm(train_data_loader)\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "    print('Training Loss : {:.5f}'.format(loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizing Predictions\n",
    "\n",
    "Using Remo, we can visualize predictions vs the original labels.\n",
    "\n",
    "To do this we create a new AnnotationSet, and upload predictions as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping Between Predicted Index and Class Name\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}\n",
    "\n",
    "detection_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "test_data_loader = tqdm.tqdm(test_data_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in test_data_loader:\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "\n",
    "            boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "            scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "            boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "            scores = scores[scores >= detection_threshold]\n",
    "            image_id = image_ids[i]\n",
    "            \n",
    "            for box, labels in zip(boxes, outputs[i]['labels']):\n",
    "                results.append({'file_name' : os.path.basename(image_id), \n",
    "                                'classes'   : mapping[labels.item()], \n",
    "                                'xmin'      : box[0],\n",
    "                                'ymin'      : box[1],\n",
    "                                'xmax'      : box[2],\n",
    "                                'ymax'      : box[3]})\n",
    "\n",
    "model_predictions_path = path_to_annotations + 'model_predictions.csv'\n",
    "            \n",
    "with open(model_predictions_path, 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['file_name', 'classes', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Using Remo, we can easily iterate through the images to compare the model predictions against the original labels.\n",
    "\n",
    "To do this, we just need to upload the model predictions to a new ```AnnotationSet```, which we call `model_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = object_detection_dataset.create_annotation_set(annotation_task='Object Detection', \n",
    "                                                             name = 'model-predictions-oid',\n",
    "                                                             paths_to_files = [train_test_split_file_path, model_predictions_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualizing the predicted boxes, the ground truth on Remo, we are able to get visual feedback on how the model is performing.\n",
    "\n",
    "- Some insights include, how the model makes an incorrect prediction of the the lamp as a wheel, perhaps due to the shape being similar.\n",
    "- Via this workflow, we can not only understand the model through its metrics, but also visually inspect its biases and iterate to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_predictions](assets/obj_visualize_results.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pt_36': conda)",
   "language": "python",
   "name": "python361064bitpt36condae3e1a72d28c3471788eb7ee1d404ab9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}