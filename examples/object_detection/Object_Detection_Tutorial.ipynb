{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![remo_logo](assets/remo_normal.png)\n",
    "\n",
    "In this tutorial, Remo will be used to accelerate the process of building a transfer learning pipeline for the task of Object Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import csv\n",
    "import json\n",
    "random.seed(4)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import remo\n",
    "remo.set_viewer('jupyter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "- The dataset used in this example is a subset of the [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html).\n",
    "\n",
    "The directory structure of the dataset is:\n",
    "\n",
    "        ├── wheat_dataset\n",
    "            ├── images\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── annotations\n",
    "                ├── wheat_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Download Cell** : goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"object_detection_dataset\"\n",
    "images_path = os.path.join(root_dir, \"images\")\n",
    "annotations_path = os.path.join(root_dir, \"annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Tags from Folders**\n",
    "\n",
    "To generate tags file for train, test, validation split, a dictionary mapping the tag to list of paths is passed to ```remo.generate_tags_from_folders()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## REQUIRES MERGE OF PULL REQUEST TO RUN\n",
    "\n",
    "im_list = [os.path.basename(i) for i in glob.glob(str(images_path)+\"/**/*.jpg\", recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "# Defining the train test split\n",
    "train_idx = round(len(im_list) * 0.4)\n",
    "valid_idx = train_idx + round(len(im_list) * 0.3)\n",
    "test_idx = valid_idx + round(len(im_list) * 0.3)\n",
    "\n",
    "# Tags Dictionary\n",
    "tags_dict = { \"train\" : im_list[0:train_idx], \n",
    "              \"valid\" : im_list[train_idx:valid_idx], \n",
    "              \"test\"  : im_list[valid_idx:test_idx] }\n",
    "\n",
    "# Generating Tags file\n",
    "remo.generate_tags_from_folders(tags_dictionary = tags_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Data to Remo**\n",
    "\n",
    "To add a dataset, you can use the ```remo.create_dataset()``` specifying the path to data and annotations. The class encoding is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported please refer the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset = remo.create_dataset(name=\"Object_Detection_Dataset\", \n",
    "                                               local_files=[root_dir, \"train_test_valid_split.csv\"], \n",
    "                                               annotation_task= \"Object Detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view your data and labels using the Remo visual interface directly in the notebook, call the ```dataset.view()``` method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_view](assets/obj_dataset_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties.\n",
    "\n",
    "This can be done either using code, or via the visual interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![view_annotations_stats](assets/obj_view_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Annotations To File**\n",
    "\n",
    "Using the ```dataset.export_annotations_to_file()``` method, the annotations from Remo can be exported to a format of your choice.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.export_annotations_to_file(\"object_detection_dataset.zip\", annotation_format=\"csv\", full_path='true')\n",
    "!unzip object_detection_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feeding Data into PyTorch\n",
    "\n",
    "A custom PyTorch Dataset object defined below is used to load data.\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "\n",
    "- **Path to Tags:** Path to Tags file for Train, Test, Validation split CSV generated by Remo\n",
    "- **Path to Annotations:** Path to Annotations CSV File (Format : file_name, classes, xmin, ymin, xmax, ymax)\n",
    "- **transforms:** Transforms to be applied to the images before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations, train_test_split, transform=None, mapping=None, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "\n",
    "        self.data = pd.read_csv(annotations)\n",
    "        self.data[\"im_name\"] = self.data[\"file_name\"].apply(lambda x : os.path.basename(x))\n",
    "        self.data = self.data.set_index(\"im_name\")\n",
    "\n",
    "        # Tags for Test Train Split\n",
    "        self.train_test_split = pd.read_csv(train_test_split).set_index(\"file_name\")\n",
    "        self.data[\"tag\"] = -1\n",
    "\n",
    "        # Update Tags using Pandas, Column im_name in self.data is compared to file_name in self.train_test_split \n",
    "        self.data.update(self.train_test_split)\n",
    "        \n",
    "        # Load only Train/Test/Split depending on the mode\n",
    "        self.data = self.data[self.data[\"tag\"] == self.mode].reset_index(drop=True)\n",
    "        \n",
    "        self.file_names = self.data['file_name'].unique()\n",
    "        self.transform = transform\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.file_names.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        file_name = self.file_names[index]\n",
    "        records = self.data[self.data['file_name'] == file_name].reset_index()\n",
    "        \n",
    "        image = np.array(Image.open(file_name), dtype=np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        if self.mode != \"test\":\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "            \n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            if self.mapping is not None:\n",
    "                labels = np.zeros((records.shape[0],))\n",
    "            \n",
    "                for i in range(records.shape[0]):\n",
    "                    labels[i] = self.mapping[records.loc[i, \"classes\"]]\n",
    "                    \n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            else:\n",
    "                labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "            \n",
    "            target = {}\n",
    "\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd \n",
    "            target['boxes'] = torch.stack(list((map(torch.tensor, target['boxes'])))).type(torch.float32)\n",
    "\n",
    "            return image, target, file_name\n",
    "        else:\n",
    "            return image, file_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a DataLoader method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Mapping between Class name and Index\n",
    "cat_to_index = {\"Wheel\" : 1, \"Car\" : 2, \"Person\" : 3, \"Land vehicle\" : 4, \n",
    "                \"Human body\" : 5, \"Plant\" : 6, \"Tire\" : 7, \"Vehicle\" : 8, \n",
    "                \"Vehicle registration plate\" : 9}\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(annotations=\"Object detection.csv\",  \n",
    "                                      train_test_split=\"tags.csv\", \n",
    "                                      transform=tensor_transform,\n",
    "                                      mapping=cat_to_index,\n",
    "                                      mode=\"train\")\n",
    "\n",
    "test_dataset = ObjectDetectionDataset(annotations=\"Object detection.csv\", \n",
    "                                      train_test_split=\"tags.csv\", \n",
    "                                      transform=tensor_transform, \n",
    "                                      mapping=cat_to_index,\n",
    "                                      mode=\"test\")\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The pre-trained ```Faster RCNN``` Model with the ```ResNet-50 Backbone``` is used in this tutorial.\n",
    "\n",
    "To train the model, the following details are specified:\n",
    "\n",
    "- **Model**: The edited version of the pre-trained model.\n",
    "- **num_classes**: The number of classes present in your dataset (Eg: num_classes + 1 (background))\n",
    "- **Optimizer:** The optimizer used for training the network\n",
    "- **Num_epochs:** The number of epochs for which we would like to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 10\n",
    "loss_value = 0.0\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_data_loader = tqdm.tqdm(train_data_loader)\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "    print(f\"\\n Epoch #{epoch} loss: {loss_value}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "For visualizing the predicted v/s original annotation in Remo, the predictions are added to a CSV, which is then added as an ```AnnotationSet``` to the Remo Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_fpn.pth'))\n",
    "model.eval()\n",
    "\n",
    "x = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping Between Predicted Index and Class Name\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}\n",
    "\n",
    "detection_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "test_data_loader = tqdm.tqdm(test_data_loader)\n",
    "\n",
    "\n",
    "for images, image_ids in test_data_loader:\n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        for box, labels in zip(boxes, outputs[i]['labels']):\n",
    "            results.append({\"file_name\" : os.path.basename(image_id), \"classes\" : mapping[labels.item()], \n",
    "            \"xmin\" : box[0], \"ymin\" : box[1], \"xmax\" : box[2], \"ymax\" : box[3]})\n",
    "\n",
    "with open('results.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"file_name\", \"classes\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_detection_dataset.create_annotation_set(\"Object Detection\", name=\"model_predictions\", path_to_annotation_file=\"./results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_predictions](assets/obj_visualize_results.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
