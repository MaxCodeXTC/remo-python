{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# MC: need to specify path to remo in notebook\n",
    "# mac version\n",
    "local_path_to_repo =  '/Users/melodi/Docs/remo-python'\n",
    "# windows version\n",
    "#local_path_to_repo =  'C:/Users/crows/Documents/GitHub/remo-python'\n",
    "\n",
    "sys.path.insert(0, local_path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    (\\(\\ \n",
      "    (>':') Remo server is running: {'version': '0.3.4'}\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import remo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset 1 - 'Vova's dataset',\n",
       " Dataset 2 - 'open images detection',\n",
       " Dataset 3 - 'Subset of OID',\n",
       " Dataset 4 - 'train',\n",
       " Dataset 6 - 'test',\n",
       " Dataset 12 - 'oid 100',\n",
       " Dataset 13 - 'open images sample data',\n",
       " Dataset 19 - 'oid building ',\n",
       " Dataset 21 - 'oid building and car',\n",
       " Dataset 22 - 'oid building car detection',\n",
       " Dataset 25 - 'open images data',\n",
       " Dataset 26 - 'oid car',\n",
       " Dataset 28 - 'oid car and person',\n",
       " Dataset 29 - 'test upload',\n",
       " Dataset 30 - 'test upload 2',\n",
       " Dataset 31 - 'cats and dogs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remo.list_datasets() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open http://localhost:8000/datasets/28\n"
     ]
    }
   ],
   "source": [
    "my_dataset = remo.get_dataset(28)\n",
    "my_dataset.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = my_dataset[0:len(my_dataset) // 3]\n",
    "train = my_dataset[len(my_dataset) // 3:]\n",
    "val.export_annotation_to_csv('val.csv')\n",
    "train.export_annotation_to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export our train and validation sets into .csv files in the format of:\n",
    "  \n",
    "    file_name,class,xmin,ymin,xmax,ymax,height,width\n",
    "\t039b687dbee6ec4a.jpg,Person,103.863296,283.794432,226.743296,588.800256,768,1024\n",
    "\t039b687dbee6ec4a.jpg,Person,383.268864,212.845824,558.811136,626.833920,768,1024\n",
    "    ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert annotation format into .xml for darknet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_row(row, class_name='car', root_path='/Users/melodi/Docs/images/', annotation_path='/Users/melodi/Docs/annots/'):\n",
    "    vals = ('train', row['file_name'], root_path + row['file_name'], str(row['width']), str(row['height']),)\n",
    "    for i in range(len(row['class'])):\n",
    "        vals = vals + (class_codes[row['class'][i]].lower(),str(row['xmin'][i]), str(row['ymin'][i]), str(row['xmax'][i]), str(row['ymax'][i]),)\n",
    "    \n",
    "    result = \"\"\"<annotation>\n",
    "    <folder>%s</folder>\n",
    "    <filename>%s</filename>\n",
    "    <path>%s</path>\n",
    "    <source>\n",
    "        <database>Unknown</database>\n",
    "    </source>\n",
    "    <size>\n",
    "        <width>%s</width>\n",
    "        <height>%s</height>\n",
    "        <depth>3</depth>\n",
    "    </size>\n",
    "    <segmented>0</segmented>\n",
    "    \"\"\" + \"\"\"<object>\n",
    "        <name>%s</name>\n",
    "        <pose>Unspecified</pose>\n",
    "        <truncated>0</truncated>\n",
    "        <difficult>0</difficult>\n",
    "        <bndbox>\n",
    "            <xmin>%s</xmin>\n",
    "            <ymin>%s</ymin>\n",
    "            <xmax>%s</xmax>\n",
    "            <ymax>%s</ymax>\n",
    "        </bndbox>\n",
    "    </object>\"\"\" * len(row['class']) +\"\"\"\n",
    "</annotation>\n",
    "\"\"\" \n",
    "    result = result % vals\n",
    "    filename = row['file_name'].split('.')[0]\n",
    "    path = annotation_path + filename\n",
    "    f =  open(path + \".xml\", \"w\")\n",
    "    f.write(result)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group by file_name and aggreagate in a list in the case of multiple objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df_grouped = df.groupby('file_name')['class'].apply(list).reset_index(name='class')\n",
    "for column in df.columns[2:]:\n",
    "    df_new = df.groupby('file_name')[column].apply(list).reset_index(name=column)\n",
    "    df_grouped = df_grouped.merge(df_new, on=['file_name'])\n",
    "# height and width is the same\n",
    "df_grouped['height'] = df_grouped['height'].apply(lambda x: x[0])\n",
    "df_grouped['width'] = df_grouped['width'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "129    None\n",
       "130    None\n",
       "131    None\n",
       "132    None\n",
       "133    None\n",
       "Length: 134, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped.apply(convert_row,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from darkflow.net.build import TFNet\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing /Users/melodi/Docs/remo-python/cfg/yolov2.cfg\n",
      "Loading /Users/melodi/Docs/remo-python/bin/yolov2.weights ...\n",
      "Successfully identified 203934260 bytes\n",
      "Finished in 1.413050889968872s\n",
      "/Users/melodi/Docs/remo-python/cfg/yolov2.cfg loss hyper-parameters:\n",
      "\tH       = 19\n",
      "\tW       = 19\n",
      "\tbox     = 5\n",
      "\tclasses = 80\n",
      "\tscales  = [1.0, 5.0, 1.0, 1.0]\n",
      "WARNING:tensorflow:From /Users/melodi/opt/anaconda3/lib/python3.7/site-packages/darkflow/net/yolov2/train.py:87: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Building /Users/melodi/Docs/remo-python/cfg/yolov2.cfg loss\n",
      "WARNING:tensorflow:From /Users/melodi/opt/anaconda3/lib/python3.7/site-packages/darkflow/net/yolov2/train.py:107: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "INFO:tensorflow:Summary name /Users/melodi/Docs/remo-python/cfg/yolov2.cfg loss is illegal; using Users/melodi/Docs/remo-python/cfg/yolov2.cfg_loss instead.\n",
      "WARNING:tensorflow:From /Users/melodi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/melodi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "options = {\"model\": \"/Users/melodi/remo-python/cfg/yolov2.cfg\", \n",
    "           \"load\": \"/Users/melodi/remo-python/bin/yolov2.weights\",\n",
    "           \"batch\": 40,\n",
    "           \"epoch\": 1,\n",
    "           \"train\": True,\n",
    "           \"verbalise\": False,\n",
    "           \"annotation\": \"/Users/melodi/Docs/annots/\",\n",
    "           \"dataset\": \"/Users/melodi/Docs/images/\",\n",
    "           \"labels\": \"/Users/melodi/remo-python/labels.txt\",\n",
    "          \"backup\": \"/Users/melodi/remo-python/ckpt/\"}\n",
    "tfnet = TFNet(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/melodi/Docs/remo-python/cfg/yolov2.cfg parsing /Users/melodi/Docs/annots/\n",
      "Parsing for ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] \n",
      "[====================>]100%  09e3c15d734763b1.xml\n",
      "Statistics:\n",
      "car: 194\n",
      "person: 296\n",
      "Dataset size: 134\n",
      "Dataset of 134 instance(s)\n"
     ]
    }
   ],
   "source": [
    "tfnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/melodi/Docs/remo-python/ckpt/yolov2-3\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    " \"model\": \"/Users/melodi/remo-python/cfg/yolov2.cfg\", \n",
    "   \"load\": 3,\n",
    " 'threshold': 0.5,\n",
    "    \"verbalise\": False,\n",
    "    \"labels\": \"/Users/melodi/remo-python/labels.txt\",\n",
    "  \"backup\": \"/Users/melodi/remo-python/ckpt/\"}\n",
    "\n",
    "tfnet2 = TFNet(options)\n",
    "tfnet2.load_from_ckpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions in Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed our validation set into the model and prepare the results to upload Remo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "cars = os.listdir('/Users/melodi/remo-python/val/Car')\n",
    "results = []\n",
    "for img_file in cars:\n",
    "    original_img = cv2.imread('/Users/melodi/remo-python/val/Car/'+img_file)\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    outputs = tfnet2.return_predict(original_img)\n",
    "    result = {'file_name':img_file, 'objects':outputs}\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = os.listdir('/Users/melodi/remo-python/val/Person')\n",
    "for img_file in people:\n",
    "    original_img = cv2.imread('/Users/melodi/remo-python/val/Person/'+img_file)\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    outputs = tfnet2.return_predict(original_img)\n",
    "    result = {'file_name':img_file, 'objects':outputs}\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output consists to a dictionary containing file_name and objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will convert this into dataframe in order to feed into Remo as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['file_name','class','xmin','ymin','xmax','ymax'])\n",
    "for res in results:\n",
    "    for i in range(len(res['objects'])):\n",
    "        df.loc[len(df)+1] = [res['file_name'],d.get(res['objects'][i]['label']).capitalize(),res['objects'][i]['topleft']['x'],res['objects'][i]['topleft']['y'], res['objects'][i]['bottomright']['x'], res['objects'][i]['bottomright']['y']]\n",
    "# drop the other classes different than car and person.\n",
    "df = df.dropna(subset=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/melodi/Docs/remo-python/example/detection_preds_raw1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will upload this .csv file into Remo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 43,\n",
       " 'name': 'Car',\n",
       " 'released_at': None,\n",
       " 'updated_at': '2020-01-29T13:15:04.234339Z',\n",
       " 'task': 1,\n",
       " 'dataset': 28,\n",
       " 'last_annotated_date': None,\n",
       " 'classes': [{'id': 34, 'name': 'Car'}, {'id': 1, 'name': 'Person'}],\n",
       " 'is_last_modified': False,\n",
       " 'type': 'image',\n",
       " 'is_public': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MC: There is a bug in annotation name assignment\n",
    "my_dataset.create_annotation_set(annotation_task='Object detection', name='Predictions',classes=['Person','Car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.add_annotations_by_csv('/Users/melodi/Docs/remo-python/example/detection_preds_raw1.csv', 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open http://localhost:8000/datasets/28\n"
     ]
    }
   ],
   "source": [
    "my_dataset.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
